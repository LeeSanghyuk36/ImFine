{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czLnIfHVRrF4"
      },
      "source": [
        "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-EHG_TRrF6"
      },
      "source": [
        "## Quick Start Notebook\n",
        "\n",
        "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
        "\n",
        "### Step 0: Install pre-requirements and convert checkpoint\n",
        "\n",
        "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
        "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
        "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIsCytZ2RrF6"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install llama-recipes transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
        "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
        "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdQN3onELGVj"
      },
      "outputs": [],
      "source": [
        "|!huggingface-cli login --token \"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/"
      ],
      "metadata": {
        "id": "qTziCWtJIhDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5F_Pu-rRrF7"
      },
      "source": [
        "### Step 1: Load the model\n",
        "\n",
        "Point model_id to model weight folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uio1TCEL17o"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.31\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufvaiEl9RrF7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id=\"meta-llama/Llama-2-7b-hf\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model = LlamaForCausalLM.from_pretrained(model_id, cache_dir=\"tmp/model\",load_in_8bit=True, device_map='auto', torch_dtype=torch.float32) # cache_dir=\"custom_cache_dir\" 로 저장 위치 지정 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zvAznrlH7z5"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw34RKVERrF8"
      },
      "source": [
        "### Step 2: Load the preprocessed dataset\n",
        "\n",
        "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15xsiHQMRrF8"
      },
      "outputs": [],
      "source": [
        "from llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n",
        "from llama_recipes.configs.datasets import samsum_dataset, alpaca_dataset, grammar_dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# train_dataset = get_preprocessed_dataset(tokenizer, samsum_dataset, 'train')\n",
        "# data_files = {\"train\": \"Dataset_Lee_2.json\"}\n",
        "# train_data = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "train_dataset = get_preprocessed_dataset(tokenizer, alpaca_dataset, 'train')\n",
        "# print(train_data)\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(train_dataset[0])"
      ],
      "metadata": {
        "id": "z7Jdez3LSfCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = train_data['train']"
      ],
      "metadata": {
        "id": "eIRgiNGkSgkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfAqRRlDH7z6",
        "outputId": "35873c80-8968-45cf-ef38-bd2aecf210eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_recipes.datasets.alpaca_dataset.InstructionDataset at 0x7e8124221030>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_dataset\n",
        "# Dataset({\n",
        "#     features: ['input_ids', 'attention_mask', 'labels'],\n",
        "#     num_rows: 1555\n",
        "# })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 토큰화 함수 정의\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# def tokenize_function(example):\n",
        "#     # 모델이 입력으로 받을 수 있는 형식으로 토큰화\n",
        "#     inputs = tokenizer(example['input'], truncation=True, padding=True)\n",
        "#     labels = tokenizer(example['output'], truncation=True, padding=True)\n",
        "\n",
        "#     # 토큰화된 결과를 리턴\n",
        "#     return {\n",
        "#         'input_ids': inputs.input_ids,\n",
        "#         'attention_mask': inputs.attention_mask,\n",
        "#         'labels': labels.input_ids  # 'labels' 역시 토큰화된 형식으로 변환\n",
        "#     }\n",
        "\n",
        "# # 토큰화된 데이터셋 생성\n",
        "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "# train_dataset"
      ],
      "metadata": {
        "id": "oS79_Xv4YWv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = train_dataset.remove_columns([\"output\", \"input\", \"instruction\"])\n",
        "# # train_dataset.set_format(\"torch\")\n",
        "# train_dataset.column_names"
      ],
      "metadata": {
        "id": "ZYjuaUOMZjRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRN2dB1RrF9"
      },
      "source": [
        "### Step 3: Check base model\n",
        "\n",
        "Run the base model on an example input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvqPVx6MRrF9"
      },
      "outputs": [],
      "source": [
        "# eval_prompt = \"\"\"\n",
        "# Summarize this dialog:\n",
        "# A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "# B: I’m pretty sure I am. What’s up?\n",
        "# A: Can you go with me to the animal shelter?.\n",
        "# B: What do you want to do?\n",
        "# A: I want to get a puppy for my son.\n",
        "# B: That will make him so happy.\n",
        "# A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "# B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "# A: I'll get him one of those little dogs.\n",
        "# B: One that won't grow up too big;-)\n",
        "# A: And eat too much;-))\n",
        "# B: Do you know which one he would like?\n",
        "# A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "# B: I bet you had to drag him away.\n",
        "# A: He wanted to take it home right away ;-).\n",
        "# B: I wonder what he'll name it.\n",
        "# A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "# ---\n",
        "# Summary:\n",
        "# \"\"\"\n",
        "\n",
        "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAiZRsATRrF9"
      },
      "source": [
        "We can see that the base model only repeats the conversation.\n",
        "\n",
        "### Step 4: Prepare model for PEFT\n",
        "\n",
        "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0cwqeLFRrF9",
        "outputId": "18674845-27a7-414f-d98d-6036f5cca12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "def create_peft_config(model):\n",
        "    from peft import (\n",
        "        get_peft_model,\n",
        "        LoraConfig,\n",
        "        TaskType,\n",
        "        prepare_model_for_int8_training,\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules = [\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "\n",
        "    # prepare int-8 model for training # 8bit 가 연산에 최적화 돼있어 추론 성능을 높이고 메모리 사용량을 줄임\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "    return model, peft_config\n",
        "\n",
        "# create peft config\n",
        "model, lora_config = create_peft_config(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dft3g4eDRrF-",
        "tags": []
      },
      "source": [
        "### Step 5: Define an optional profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMFiZAYvRrF-"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "from contextlib import nullcontext\n",
        "enable_profiler = False\n",
        "output_dir = \"tmp/llama-output\"\n",
        "\n",
        "config = {\n",
        "    'lora_config': lora_config,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_train_epochs': 1,\n",
        "    'gradient_accumulation_steps': 2,\n",
        "    'per_device_train_batch_size': 2,\n",
        "    'gradient_checkpointing': False,\n",
        "}\n",
        "\n",
        "# Set up profiler\n",
        "if enable_profiler:\n",
        "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
        "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
        "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
        "    profiler = torch.profiler.profile(\n",
        "        schedule=schedule,\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True)\n",
        "\n",
        "    class ProfilerCallback(TrainerCallback):\n",
        "        def __init__(self, profiler):\n",
        "            self.profiler = profiler\n",
        "\n",
        "        def on_step_end(self, *args, **kwargs):\n",
        "            self.profiler.step()\n",
        "\n",
        "    profiler_callback = ProfilerCallback(profiler)\n",
        "else:\n",
        "    profiler = nullcontext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORXhbjdRrF-"
      },
      "source": [
        "### Step 6: Fine tune the model\n",
        "\n",
        "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TKpI2nWRrF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7043e7a7-bfeb-41ff-e1ca-3ca56fdd6e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<llama_recipes.datasets.alpaca_dataset.InstructionDataset object at 0x7e8124221030>\n"
          ]
        }
      ],
      "source": [
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    bf16=False,  # Use BF16 if available\n",
        "    # logging strategies\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    max_steps=total_steps if enable_profiler else -1,\n",
        "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
        ")\n",
        "\n",
        "with profiler:\n",
        "    # Create Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=default_data_collator,\n",
        "        callbacks=[profiler_callback] if enable_profiler else [],\n",
        "    )\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_RtwwGvrq_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "855a75d5-c938-4438-aca0-068716067260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:47, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.227300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13, training_loss=1.209174082829402, metrics={'train_runtime': 51.6798, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.252, 'total_flos': 453181255188480.0, 'train_loss': 1.209174082829402, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcHFBdn4H7z7"
      },
      "source": [
        "### Step 7: Save model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WidM32DaRrF-"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ixmjYd3RrF-"
      },
      "source": [
        "### Step 8: Try the fine tuned model on the same example again to see the learning progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y30WdpiRRrF-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6ae67d04-cb46-48e5-819e-075ce5988585"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-89c799044764>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_input' is not defined"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udn2MkmbN8Pj"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "A: 야 뭐하냐\n",
        "B: 밥먹어\n",
        "A: 무슨 메뉴\n",
        "B: 김치찌개\n",
        "A: 어제도 먹지 않았어?\n",
        "B: 내가 김치매니아라서 또 먹는데 불만있냐\n",
        "A: 아니 근데 나는 김치 안좋아함\n",
        "B: 그럼 딴거 먹어\n",
        "A: ㅇㅇ 돈까스 먹어야겠다\n",
        "---\n",
        "요약\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8ethvjYH7z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4fc5a8-21b8-4c70-b789-de195fbeb8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            " ### Instruction: How did ediacaran fauna differ from cambrian fauna? ### output: 1. ediacaran fauna were simpler in structure and smaller in size. 2. ediacaran fauna were more complex in structure and larger in size.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n ### Instruction: How did ediacaran fauna differ from cambrian fauna? ### output: \"\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl-hHo_QH7z8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "V100",
      "language": "python",
      "name": "v100"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}