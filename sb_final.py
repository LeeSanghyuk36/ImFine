# -*- coding: utf-8 -*-
"""SB_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QV2DKacxG3izMWp02QESGRe1En05SHo2
"""

for layer_name, layer in model_anime_1990s.items():
  if("weight" in layer_name and "embeddings" not in layer_name):
    findMinMax(layer_name, model_anime_1990s, model_anime_ip)

list_layers = []

for layer_name in model_anime_1990s:
  list_layers.append(layer_name)

list_layers

print(len(list_layers))

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

import platform

# 현재 운영 체제 정보 가져오기
os_info = platform.platform()

# 운영 체제 정보 출력
print("현재 운영 체제:", os_info)

!lsb_release -a

import torch
print("cudnn version:{}".format(torch.backends.cudnn.version()))
print("cuda version: {}".format(torch.version.cuda))
if torch.cuda.is_available():
    # 하나 이상의 GPU가 사용 가능한 경우
    print(f"사용 가능한 GPU 수: {torch.cuda.device_count()}")
    print(f"현재 활성화된 GPU 디바이스: {torch.cuda.current_device()}")
    print(f"사용 가능한 GPU 디바이스 목록:\n{[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}")
else:
    print("사용 가능한 GPU가 없습니다.")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"현재 사용 중인 디바이스: {device}")

import torch

if torch.cuda.is_available():
    print("GPU 사용 가능")
else:
    print("GPU 사용 불가능")

!python --version

!pip install diffusers["torch"] transformers

!pip install safetensors

from safetensors.torch import load_file

# !pip install diffusers==0.11.1
!pip install transformers scipy ftfy accelerate
!pip install diffusers

from google.colab import files

import json
import chardet  # chardet 모듈을 설치해야 합니다. (pip install chardet)

def convert_safetensor_to_json(input_file_path, output_directory):
    try:
        # 파일 인코딩을 자동으로 감지합니다.
        with open(input_file_path, 'rb') as safetensor_file:
            raw_data = safetensor_file.read()
            detected_encoding = chardet.detect(raw_data)['encoding']

        # 감지된 인코딩으로 파일을 다시 엽니다.
        with open(input_file_path, 'r', encoding=detected_encoding) as safetensor_file:
            safetensor_data = safetensor_file.read()

        # .safetensor 데이터를 JSON으로 파싱합니다.
        try:
            safetensor_json = json.loads(safetensor_data)
        except json.JSONDecodeError:
            return "파일을 JSON으로 파싱할 수 없습니다."

        # 새로운 .json 파일의 경로 생성
        input_filename = input_file_path.split('/')[-1]
        output_file_path = f'{output_directory}/{input_filename.replace(".safetensor", ".json")}'

        # .json 파일로 저장합니다.
        with open(output_file_path, 'w') as json_file:
            json.dump(safetensor_json, json_file, indent=4)

        return output_file_path

    except FileNotFoundError:
        return "입력 파일을 찾을 수 없습니다."

# # 사용 예시
# input_file_path = '/content/drive/MyDrive/v1-5-pruned-emaonly.safetensors'
# output_directory = '/content/drive/MyDrive/'
# result = convert_safetensor_to_json(input_file_path, output_directory)
# if result:
#     print(f'변환된 .json 파일이 저장되었습니다: {result}')
# else:
#     print('변환 실패')

from google.colab import drive
drive.mount('/content/drive')

def printLayerNameAndLayerSize(model):
  items_loaded = []
  for layer_name, layer in model.items():
      print(f"Layer Name: {layer_name} Layer Size : {layer.size()}")
      items_loaded.append(layer_name)
  return items_loaded

!pip install stable_diffusion

import torch
# from stableDiffusion import DiffusionPipeline

from diffusers import DiffusionPipeline

!pip install stable-diffusion

import torch
from diffusers import StableDiffusionPipeline


filepath = '/content/drive/MyDrive/v1-5-pruned-emaonly.safetensors'
model_base = load_file(filepath)

filepath = '/content/drive/MyDrive/helloflatart_V12a.safetensors'
model_colorful_hello = load_file(filepath)

filepath = '/content/drive/MyDrive/restlessexistence_v30Reflection.safetensors'
model_colorful_restless = load_file(filepath)

filepath = '/content/drive/MyDrive/1990s2000s_v10.safetensors'
model_anime_1990s =  load_file(filepath)

filepath = '/content/drive/MyDrive/ipDESIGN3D_v20.safetensors'
model_anime_ip =  load_file(filepath)

filepath = '/content/drive/MyDrive/beautifulRealistic_v7.safetensors'
model_raceful_asian =  load_file(filepath)

filepath = '/content/drive/MyDrive/cyberrealistic_v40.safetensors'
model_raceful_cyber =  load_file(filepath)

from diffusers import EulerDiscreteScheduler

pretrained_model_path = "/content/drive/MyDrive/1990s2000s_v10.safetensors"

pre_pipelint = DiffusionPipeline.from_pretrained(pretrained_model_path ,torch_dtype=torch.float16,use_safetensors=True)

scheduler = EulerDiscreteScheduler.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="scheduler")
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt


def makePicture(model, prompt, negative_prompt):
  # StableDiffusionPipeline을 생성합니다.
  pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", use_safetensors=True)
  # pipeline = DiffusionPipeline.from_pretrained("/content/drive/MyDrive/v1-5-pruned-emaonly.safetensors",torch_dtype=torch.float16,use_safetensors=True)
  # scheduler 변경
  pipe.scheduler = scheduler

  # pretrained 모델을 pipe에 설정합니다.
  pipe.model = model

  # pipe.scheduler = "E"
  # pipe.callback_stage = 50


  pipe = pipe.to("cuda")
  pipe.enable_attention_slicing()

  # 이미지를 생성합니다.
  images = pipe(prompt, num_inference_steps=25, height=512, width=512, guidance_scale=7, negative_prompt=negative_prompt).images

  # # 이미지를 5 * 5 격자로 출력합니다.
  # image_grid = np.concatenate(images, axis=1)
  # image_grid = np.concatenate([image_grid] * 5, axis=0)

  # # 이미지를 한번에 보여줍니다.
  # plt.imshow(image_grid)
  # plt.show()

  return images

makePicture(model_anime_1990s, "a pretty cat", "low quality, worst quality")

from PIL import Image

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

# import torch
# scheduler = EulerDiscreteScheduler.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="scheduler")

# # checkpoint 모델을 로드합니다.
# checkpoint = load_file("/content/drive/MyDrive/v1-5-pruned-emaonly.safetensors")


# # model을 생성합니다.
# # model = DiffusionPipeline.from_checkpoint(checkpoint, use_safetensors=True)

# model.scheduler = scheduler


# prompt = "face Close-up,photo of a sexy Korean idol,face,Detailed pupils,skin texture,From front,Detailed eyes,sexy, seductive,necklaces, earrings, jewelry,eye shadow,skin pores"
# negative_prompt = "(worst quality,low quality,illustration,3d,2d,painting,cartoons,sketch),(blurry)"


# # 이미지를 생성합니다.
# image = model(prompt=prompt, num_inference_steps=25, height=512, width=512, guidance_scale=7, negative_prompt=negative_prompt).images[0]

# # 이미지를 저장합니다.
# image

def flatten_torch_tensor(tensor):
  """tensor를 1차원 배열로 반환합니다.

  Args:
    tensor: tensor.

  Returns:
    1차원 배열.
  """

  # tensor의 차원을 확인합니다.
  ndim = tensor.ndim

  # 1차원 배열로 변환합니다.
  if ndim == 1:
    return tensor
  elif ndim == 2:
    return tensor.reshape(-1)
  elif ndim == 3:
    return flatten_torch_tensor(tensor.reshape(-1, tensor.shape[-1] * tensor.shape[-2]))
  elif ndim == 4:
    return flatten_torch_tensor(tensor.reshape(-1, tensor.shape[-1] * tensor.shape[-2] * tensor.shape[-3]))
  else:
    raise ValueError(f"Unsupported ndim: {ndim}")

def findMinMax(layer_name , model1, model2):
  minNum = 999999
  maxNum = -99999
  sumNum = 0
  flatten_model_model1 = flatten_torch_tensor(model1[layer_name])
  flatten_model_model2 = flatten_torch_tensor(model2[layer_name])

  for i in range(flatten_model_model2.size()[0]):
    diff = flatten_model_model1[i].item() - flatten_model_model2[i].item()
    if(diff > maxNum):
      maxNum = diff
    if(diff < minNum):
      minNum = diff

  print("-------------------------------------------------------------------")
  print(layer_name)
  if((maxNum-minNum) > 0.02):
    print(f"Size : {flatten_model_model1.size()}")
    print(f"Max value of diff_arr: {maxNum}")
    print(f"Min value of diff_arr: {minNum}")

tensor = torch.randn((512, 512, 2, 3))
tensor_flatten = flatten_torch_tensor(tensor)
print(tensor_flatten.size())

len(printLayerNameAndLayerSize(model_anime_1990s))

import matplotlib.pyplot as plt

def draw_heatmap(tensor):
  """tensor를 heatmap으로 표현합니다.

  Args:
    tensor: heatmap으로 표현할 tensor.

  Returns:
    heatmap을 그린 그래프.
  """
  tensor = tensor[:, :, 0, 0]

  # 히트맵의 색상 스케일을 설정합니다.
  # cmap = plt.cm.get_cmap("jet", 256)
  norm = plt.Normalize(tensor.min(), tensor.max())

  # 히트맵을 그립니다.
  # plt.imshow(tensor.squeeze(), cmap=cmap, norm=norm)
  plt.imshow(tensor.squeeze(), norm=norm)

  # 색 범위를 표시합니다.
  # cbar = plt.colorbar(norm)

  # x축과 y축의 눈금을 설정합니다.
  plt.xlabel("x")
  plt.ylabel("y")

  # 그래프를 출력합니다.
  plt.show()

draw_heatmap(model_colorful_hello["first_stage_model.encoder.mid.attn_1.k.weight"])

draw_heatmap(model_colorful_restless["first_stage_model.encoder.mid.attn_1.k.weight"])

def printLayerNameAndLayerSize(model):
  items_loaded = []
  for layer_name, layer in model.items():
      print(f"Layer Name: {layer_name} Layer Size : {layer.size()}")
      items_loaded.append(layer_name)
  return items_loaded

flatten_model_real = flatten_torch_tensor(model_base["first_stage_model.encoder.mid.attn_1.k.weight"])
flatten_model_real.size()[0]

"""정규화 -> 병렬화 -> 대입하고 -> 실행

차이를 시각화하는 것

이러한 새로운 모델링을 적용시켜서 그림을 생성해 주는 것
"""

change_layers = [
    "cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight"
    , "cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight"
    , "first_stage_model.decoder.conv_in.weight"
    , "first_stage_model.decoder.mid.attn_1.k.weight"
    , "first_stage_model.decoder.mid.attn_1.norm.weight"
    , "first_stage_model.decoder.mid.attn_1.proj_out.weight"
    , "first_stage_model.decoder.mid.attn_1.q.weight"
    , "first_stage_model.decoder.mid.attn_1.v.weight"
    , "first_stage_model.decoder.mid.block_1.conv1.weight"
    , "first_stage_model.decoder.mid.block_1.conv2.weight"
    , "first_stage_model.decoder.mid.block_1.norm1.weight"
    , "first_stage_model.decoder.mid.block_1.norm2.weight"
    , "first_stage_model.decoder.mid.block_2.conv1.weight"
    , "first_stage_model.decoder.mid.block_2.conv2.weight"
    , "first_stage_model.decoder.mid.block_2.norm1.weight"
    , "first_stage_model.decoder.mid.block_2.norm2.weight"
    , "first_stage_model.decoder.norm_out.weight"
    , "first_stage_model.decoder.up.0.block.0.conv1.weight"
    , "first_stage_model.decoder.up.0.block.0.conv2.weight"
    , "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight"
    , "first_stage_model.decoder.up.0.block.0.norm1.weight"
    , "first_stage_model.decoder.up.0.block.0.norm2.weight"
    , "first_stage_model.decoder.up.0.block.1.conv1.weight"
    , "first_stage_model.decoder.up.0.block.1.conv2.weight"
    , "first_stage_model.decoder.up.0.block.1.norm1.weight"
    , "first_stage_model.decoder.up.0.block.1.norm2.weight"
    , "first_stage_model.decoder.up.0.block.2.conv1.weight"
    , "first_stage_model.decoder.up.0.block.2.conv2.weight"
    , "first_stage_model.decoder.up.0.block.2.norm1.weight"
    , "first_stage_model.decoder.up.0.block.2.norm2.weight"
    , "first_stage_model.decoder.up.1.block.0.conv1.weight"
    , "first_stage_model.decoder.up.1.block.0.conv2.weight"
    , "first_stage_model.decoder.up.1.block.0.nin_shortcut.weight"
    , "first_stage_model.decoder.up.1.block.0.norm1.weight"
    , "first_stage_model.decoder.up.1.block.0.norm2.weight"
    , "first_stage_model.decoder.up.1.block.1.conv1.weight"
    , "first_stage_model.decoder.up.1.block.1.conv2.weight"
    , "first_stage_model.decoder.up.1.block.2.conv1.weight"
    , "first_stage_model.decoder.up.1.block.2.conv2.weight"
    , "first_stage_model.decoder.up.1.block.2.norm1.weight"
    , "first_stage_model.decoder.up.1.block.2.norm2.weight"
    , "first_stage_model.decoder.up.1.upsample.conv.weight"
    , "first_stage_model.decoder.up.2.block.0.conv1.weight"
    , "first_stage_model.decoder.up.2.block.0.conv2.weight"
    , "first_stage_model.decoder.up.2.block.0.norm1.weight"
    , "first_stage_model.decoder.up.2.block.0.norm2.weight"
    , "first_stage_model.decoder.up.2.block.1.conv1.weight"
    , "first_stage_model.decoder.up.2.block.1.conv2.weight"
    , "first_stage_model.decoder.up.2.block.1.norm1.weight"
    , "first_stage_model.decoder.up.2.block.1.norm2.weight"
    , "first_stage_model.decoder.up.2.block.2.conv1.weight"
    , "first_stage_model.decoder.up.2.block.2.conv2.weight"
    , "first_stage_model.decoder.up.2.block.2.norm1.weight"
    , "first_stage_model.decoder.up.2.block.2.norm2.weight"
    , "first_stage_model.decoder.up.2.upsample.conv.weight"
    , "first_stage_model.decoder.up.3.block.0.conv1.weight"
    , "first_stage_model.decoder.up.3.block.0.conv2.weight"
    , "first_stage_model.decoder.up.3.block.0.norm1.weight"
    , "first_stage_model.decoder.up.3.block.0.norm2.weight"
    , "first_stage_model.decoder.up.3.block.1.conv1.weight"
    , "first_stage_model.decoder.up.3.block.1.conv2.weight"
    , "first_stage_model.decoder.up.3.block.1.norm1.weight"
    , "first_stage_model.decoder.up.3.block.1.norm2.weight"
    , "first_stage_model.decoder.up.3.block.2.conv1.weight"
    , "first_stage_model.decoder.up.3.block.2.conv2.weight"
    , "first_stage_model.decoder.up.3.block.2.norm1.weight"
    , "first_stage_model.decoder.up.3.block.2.norm2.weight"
    , "first_stage_model.decoder.up.3.upsample.conv.weight"
    , "first_stage_model.post_quant_conv.weight"
    , "first_stage_model.quant_conv.weight"
    , "model.diffusion_model.input_blocks.1.1.proj_out.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.input_blocks.10.0.in_layers.2.weight"
    , "model.diffusion_model.input_blocks.10.0.out_layers.0.weight"
    , "model.diffusion_model.input_blocks.10.0.out_layers.3.weight"
    , "model.diffusion_model.input_blocks.2.1.proj_out.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.input_blocks.4.1.proj_in.weight"
    , "model.diffusion_model.input_blocks.4.1.proj_out.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.input_blocks.5.1.proj_in.weight"
    , "model.diffusion_model.input_blocks.5.1.proj_out.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.output_blocks.10.1.proj_out.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.output_blocks.2.0.emb_layers.1.weight"
    , "model.diffusion_model.output_blocks.3.0.emb_layers.1.weight"
    , "model.diffusion_model.output_blocks.3.1.proj_in.weight"
    , "model.diffusion_model.output_blocks.3.1.proj_out.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight"
    , "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight"
    , "model.diffusion_model.output_blocks.4.0.in_layers.2.weight"
]



def unflatten_torch_tensor(tensor, base_model):
  """tensor를 shape에 맞는 배열로 반환합니다.

  Args:
    tensor: tensor.
    shape: reshape할 shape.

  Returns:
    reshape된 배열.
  """
  # reshape합니다.
  return tensor.reshape(base_model.size())

model_base["model.diffusion_model.output_blocks.9.0.in_layers.2.weight"]

import copy

model_finish = copy.deepcopy(model_base)

model_finish

model_finish["model.diffusion_model.output_blocks.9.0.in_layers.2.weight"]

import copy

def makeModel(tone, style, reality , model_copy):

  for i in range(len(change_layers)):
    layer_name = change_layers[i]
    model_size = model_finish[layer_name].size()
    # print("layer_name : " + layer_name + " model_size : " + str(model_size))

    serialize_finish = model_finish[layer_name]
    serialize_color_hello = model_colorful_hello[layer_name]
    serialize_color_restless = model_colorful_restless[layer_name]
    serialize_anime_1990 = model_anime_1990s[layer_name]
    serialize_anime_ip = model_anime_ip[layer_name]
    serialize_raceful_asian = model_raceful_asian[layer_name]
    serialize_raceful_cyber = model_raceful_cyber[layer_name]

    for j in range(serialize_finish.size()[0]):
      model_copy[j] = ((1-tone) * serialize_color_hello[j] + tone * serialize_color_restless[j])/3 + ((1-style) * serialize_anime_1990[j] + style * serialize_anime_ip[j])/3 + ((1-reality) *
      serialize_raceful_asian[j] + reality * serialize_raceful_cyber[j])/3

  return model_copy

# prompt를 설정합니다.
model_copy = copy.deepcopy(model_base)
tone = 0.2
style = 0.5
reality = 0.25

num_images = 5
prompt = ["a pretty cat"] * num_images
negative_prompt = ["worst quality, lower quality"] * num_images

images = makePicture(makeModel(tone, style, reality, model_copy), prompt, negative_prompt)

grid = image_grid(images, rows=1, cols=5)
grid

"""## Input : tone, style, reality, prompt, negative prompt 를 입력하면 모델을 만들고 5개의 이미지를 만드는 함수"""

def configuration_model_and_make_picture(tone, style, reality, prompt_message, negative_prompt_message):
  # prompt를 설정합니다.
  model_copy = copy.deepcopy(model_base)
  prompt = [prompt_message] * 5
  negative_prompt = [negative_prompt_message] * 5

  images = makePicture(makeModel(tone, style, reality, model_copy), prompt, negative_prompt)

  grid = image_grid(images, rows=1, cols=5)
  return grid

"""## Case : tone =2 , overRange, 범위를 벗어나게 입력"""

configuration_model_and_make_picture(2,0.5,0.5, "a pretty cat", "worst quality, lower quality")

"""## base 모델 : 0.5 , 0.5 , 0,5"""

configuration_model_and_make_picture(0.5,0.5,0.5, "a pretty cat , high quality", "worst quality, lower quality")

"""## Case : tone = 0, 단색의 이미지를 출력하는 기능"""

configuration_model_and_make_picture(0.0,0.5,0.5, "a pretty cat", "worst quality, lower quality")

"""## Case : tone = 1,  좀 더 다양한 색으로 표현"""

configuration_model_and_make_picture(1,0.5,0.5, "a pretty cat", "worst quality, lower quality")

"""## Case : style =  0  , 평면적인 이미지가 출력 되어야 함"""

configuration_model_and_make_picture(0.5,0,0.5, "a pretty cat", "worst quality, lower quality")

"""## Case : style = 1 , 그림이 좀 더 입체적으로 변함"""

configuration_model_and_make_picture(0.5,1,0.5, "a pretty cat", "worst quality, lower quality")

"""## Case : region = 0 , 그림이 동양적 특성을 가짐"""

configuration_model_and_make_picture(0.5,0.5,0, "a pretty cat", "worst quality, lower quality")

"""## Case : region = 1, 그림이 서양적 특성을 가지고 있음"""

configuration_model_and_make_picture(0.5,0.5,1, "a pretty cat", "worst quality, lower quality")

"""## Case tone = 0.75, style = 0.25, region =0.5"""

configuration_model_and_make_picture(0.75,0.25,0.5, "a pretty cat", "worst quality, lower quality")

"""## Case : tone = 1, style = 1, region = 0.5"""

configuration_model_and_make_picture(1,1,0.5, "a pretty cat", "worst quality, lower quality")

